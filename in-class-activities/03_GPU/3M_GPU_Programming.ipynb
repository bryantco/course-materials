{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1_E7kjUJ6IU_NL2jzGjMIBx2dz4q_nvcz?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "# Introduction to GPU Programming with Numba and CuPy\n",
        "## Estimating Pi with Monte Carlo Simulation\n",
        "\n",
        "In this notebook, we'll walk through how we can use packages with friendly `numpy`-style syntax -- `numba` and `cupy` -- to accelerate our embarrasingly parallel code using GPUs. Our goal today will be to a estimate a value -- $\\pi$ -- [using Monte Carlo Simulation](https://en.wikipedia.org/wiki/Monte_Carlo_method#Overview) (a classic task for demonstrating HPC speedups and applicable to any numerical estimation task in the social sciences).\n",
        "\n",
        "First taking a look at the hardware we have available to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqQOzobAMTla",
        "outputId": "67f08e97-96f3-472f-cac3-c331122b35fa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Mar 29 15:02:37 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7_272Nn5NU7"
      },
      "source": [
        "Serially, we can solve this problem in `numpy` like so (generating 100m coordinates in a unit square and identifying whether they fall in a unit circle or not):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9yPcgxt7-bs",
        "outputId": "0482fe33-666b-4449-d8d4-96c1e5228349",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pi Estimate:  3.141506\n",
            "Time Elapsed:  2.7714076042175293\n"
          ]
        }
      ],
      "source": [
        "# NumPy Pi Estimation with Monte Carlo Simulation\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "n_runs = 10 ** 8\n",
        "\n",
        "# Simulate Random Coordinates in Unit Square:\n",
        "ran = np.random.uniform(low=-1, high=1, size=(2, n_runs))\n",
        "\n",
        "# Identify Random Coordinates that fall within Unit Circle and count them\n",
        "result = ran[0] ** 2 + ran[1] ** 2 <= 1\n",
        "n_in_circle = np.sum(result)\n",
        "\n",
        "# Estimate Pi\n",
        "print(\"Pi Estimate: \", 4 * n_in_circle / n_runs)\n",
        "\n",
        "print(\"Time Elapsed: \", time.time() - t0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KNBM2QC_0Ys"
      },
      "source": [
        "## Vectorization on GPUs with `numba`\n",
        "\n",
        "In addition to compiling code for CPUs, `numba` also gives us the ability to compile (rudimentary CUDA) code for GPUs via the familiar `vectorize` decorator, as well as a `reduce` decorator that we have not yet seen.\n",
        "\n",
        "*Note: `numba` has a GPU random number generator and can compile GPU code via JIT (such as pi estimation code), but this is a bit more involved and beyond the scope of the class (it involves more CUDA-style syntax). [See the documentation]((https://numba.readthedocs.io/en/stable/cuda/random.html)) for more detail if you are interested.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "83C_SSyPiWoV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from numba import vectorize, cuda\n",
        "from numba.core.errors import NumbaPerformanceWarning\n",
        "import warnings\n",
        "warnings.simplefilter('ignore', category=NumbaPerformanceWarning)\n",
        "\n",
        "n_runs = 10**8\n",
        "ran = np.random.uniform(low=-1, high =1, size=(2, n_runs)).astype(np.float32)\n",
        "x, y = ran[0], ran[1]\n",
        "\n",
        "@vectorize(['i4(f4, f4)'], target='cuda')\n",
        "def in_circle(x, y):\n",
        "  '''\n",
        "  Vectorized function takes in x, y coordinates (float32, float32) within an\n",
        "  array and returns a boolean indication of whether these values are (1) or\n",
        "  are not (0) in the unit circle (cast as int32). \n",
        "  \n",
        "  All computation is done on the GPU.\n",
        "  '''\n",
        "  in_circle = x**2 + y**2 <= 1\n",
        "  return in_circle\n",
        "\n",
        "@cuda.reduce\n",
        "def gpu_sum(a, b):\n",
        "  '''\n",
        "  Sums values in an array together on the GPU, using numba's built-in\n",
        "  `reduce` algorithm.\n",
        "  '''\n",
        "  return a + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8WySv_eC27e"
      },
      "source": [
        "First, we might try to only perform the mapping operation on the GPU (passing the x and y arrays over to the GPU, computing whether the random coordinates are in the unit circle or not, and then sending values back to the CPU to sum and estimate pi). Note that `numba` accepts `numpy` arrays directly and we do not need to perform any (explicit) conversion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yptujAnIxrTI",
        "outputId": "67515d58-d644-42a8-f664-6911ebca0344",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "343 ms ± 16.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "result = in_circle(x, y) # send numpy arrays x & y to GPU and perform in_circle computation on GPU\n",
        "4 * np.sum(result) / n_runs # send resulting array back to CPU to compute pi using `numpy`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDbMYIDMC5Rs"
      },
      "source": [
        "Can also perform the summation on the GPU..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06KXRiQptIyO",
        "outputId": "a34c44df-d5c2-4ac3-eb46-2487d7e09af3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "286 ms ± 23.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "result = in_circle(x, y)\n",
        "4 * gpu_sum(result) / n_runs # perform sum on GPU this time and then only two scalar ops on CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlvU-5ftC6Xv"
      },
      "source": [
        "And store intermediate results on the GPU (`result` is currently being sent back to CPU):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-Lx56b2tPeM",
        "outputId": "a11cb6fa-350f-4700-a9ad-2847bb88dccc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "206 ms ± 31.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "# Slight improvement by keeping intermediate result on GPU, but not to the\n",
        "# degree of custom CUDA kernel.\n",
        "in_circle_dev = cuda.device_array(shape=(n_runs,), dtype=np.float32) # allocate spot in memory on GPU for output array to live\n",
        "in_circle(x, y, out=in_circle_dev) # write results of `in_circle` out to spot in memory allocated to `in_circle_dev`\n",
        "4 * gpu_sum(in_circle_dev) / n_runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4WOVLT7_RmG"
      },
      "source": [
        "Note that we see even more improved performance by avoiding passing data between CPU and GPU unless absolutely necessary. This capacity to define custom elementwise mapping functions via `@vectorize` can be particularly useful for accelerating existing `numpy` code with minimal tweaks needed. \n",
        "\n",
        "## CuPy\n",
        "\n",
        "Another useful approach for working with array data on GPUs is to use `CuPy`, which replicates much of the functionality of `numpy`, but on GPUs. You can take a look at [the documentation](https://docs.cupy.dev/en/stable/user_guide/basic.html) for more detail on all of the GPU functions that are available in CuPy. If you need to write a function that is not supported in CuPy, you can either [define a your own GPU kernel](https://docs.cupy.dev/en/stable/user_guide/basic.html) (beyond the scope of the class, as this requires some exposure to underlying CUDA syntax), or use `numba` to compile a kernel for you. Note that [CuPy arrays are also communicable via mpi4py](https://docs.cupy.dev/en/stable/user_guide/interoperability.html#mpi4py) in a distributed GPU context.\n",
        "\n",
        "To estimate $\\pi$ for instance, we can generate random numbers on our GPU in exactly the same way we did in `numpy` (but our data is now on our GPU and doesn't need to be communicated over):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "18MxzEW7CDFz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "x, y = cp.random.uniform(low=-1, high=1, size=(2, n_runs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdQoTRHiCj3t"
      },
      "source": [
        "...And check to see if if these random coordinates are in our circle or not using identical notation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z0g4eYACmGc",
        "outputId": "8afdcf90-2118-4597-b509-388756579091",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(3.14150016)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = x ** 2 + y ** 2 <= 1 # on GPU\n",
        "n_in_circle = cp.sum(result) # on GPU\n",
        "pi = 4 * n_in_circle / n_runs # on GPU\n",
        "pi.get() # explicitly send estimated value back to our CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuaibdXZDKfG"
      },
      "source": [
        "CuPy also conveniently includes a benchmarking function that explicitly times how much work is being done on GPU versus CPU. We can see, for instance, that if we run the same code and `.get()` or result back on our CPU, there is a significant amount of time that is spent on CPU operations (reinforcing the need to minimize data transfer between CPU and GPU when possible!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhDbYq3WMZMv",
        "outputId": "89e44c13-d6db-4c31-a33e-0be8ccf26877",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cupy_pi_est         :    CPU:175934.926 us   +/-1286.304 (min:173277.126 / max:177631.907) us     GPU-0:175965.796 us   +/-1286.216 (min:173310.913 / max:177664.322) us"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from cupyx.profiler import benchmark\n",
        "\n",
        "def cupy_pi_est(x, y):\n",
        "  # Compute on GPU via CuPy arrays: ~173k us (microseconds)\n",
        "  result = x ** 2 + y ** 2 <= 1\n",
        "  n_in_circle = cp.sum(result)\n",
        "  pi = 4 * n_in_circle / n_runs\n",
        "\n",
        "  # Get pi back to host CPU from GPU: ~173k us (microseconds)\n",
        "  # half of time is spent just getting pi back to host CPU from GPU device\n",
        "  pi_cpu = pi.get()\n",
        "  \n",
        "  return pi_cpu\n",
        "\n",
        "# Similar to numba performance without storing intermediate results on GPU ~300 ms\n",
        "benchmark(cupy_pi_est, (x, y), n_repeat=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoFGszNhD3Ny"
      },
      "source": [
        "Whereas if we perform the same operation without bringing the estimated value back to the CPU via `.get()`, our GPU time remains the same, but there is quite a bit less time spent overall (since there is no data transfer time):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_xGgU1TDn5w",
        "outputId": "b7b1565c-9c63-4100-c391-0f7cdf80ec46",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "cupy_pi_est         :    CPU:  615.064 us   +/-56.303 (min:  543.270 / max:  748.027) us     GPU-0:177046.494 us   +/-922.323 (min:175527.969 / max:178503.677) us"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def cupy_pi_est(x, y):\n",
        "  # Compute on GPU via CuPy arrays: ~173k us (microseconds)\n",
        "  result = x ** 2 + y ** 2 <= 1\n",
        "  n_in_circle = cp.sum(result)\n",
        "  pi = 4 * n_in_circle / n_runs\n",
        "\n",
        "  # don't send result back to CPU here for timing purposes\n",
        "  \n",
        "  return pi\n",
        "\n",
        "# Similar to numba performance without storing intermediate results on GPU ~300 ms\n",
        "benchmark(cupy_pi_est, (x, y), n_repeat=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
