{"cells":[{"attachments":{},"cell_type":"markdown","id":"44c6ad3c","metadata":{},"source":["## Introduction to PySpark on an EMR Cluster\n","\n","[This notebook is meant to be run on an AWS EMR Cluster with Spark installed; follow the instructions in `emr_cheatsheet.md` to set up such a cluster]\n","\n","Let's first take a look at the Spark version that we have available within our EMR cluster:"]},{"cell_type":"code","execution_count":1,"id":"5c4bfd22","metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting Spark application\n"]},{"data":{"text/html":["<table>\n","<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1682797367375_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-31-148.ec2.internal:20888/proxy/application_1682797367375_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-20-247.ec2.internal:8042/node/containerlogs/container_1682797367375_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["SparkSession available as 'spark'.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["'3.3.1-amzn-0'"]}],"source":["sc.version"]},{"attachments":{},"cell_type":"markdown","id":"5e11de6d","metadata":{},"source":["Suppose we have a standard Python list:"]},{"cell_type":"code","execution_count":2,"id":"5c957e7c","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"}],"source":["lst = [i for i in range(100)]"]},{"attachments":{},"cell_type":"markdown","id":"811720cb","metadata":{},"source":["Let's create an RDD from this list (with as many partitions as we have nodes by default) to demonstrate some basic PySpark operations:"]},{"cell_type":"code","execution_count":3,"id":"e4d5bcb2","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"}],"source":["lst_p = sc.parallelize(lst)"]},{"cell_type":"code","execution_count":4,"id":"89f3264b","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["2"]}],"source":["lst_p.getNumPartitions()"]},{"cell_type":"code","execution_count":5,"id":"4860a9e2","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"]}],"source":["lst_p"]},{"attachments":{},"cell_type":"markdown","id":"3cc56cb1","metadata":{},"source":["Once we have an RDD, we can perform \"transformations\" on it (such as `map` and `filter`):"]},{"cell_type":"code","execution_count":7,"id":"d78dab12","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["PythonRDD[1] at RDD at PythonRDD.scala:53"]}],"source":["filtered = lst_p.filter(lambda x: x < 10) # transformation 1\n","filtered"]},{"cell_type":"code","execution_count":8,"id":"5c8b8794","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["PythonRDD[2] at RDD at PythonRDD.scala:53"]}],"source":["mapped = filtered.map(lambda x: x * 10) # transformation 2\n","mapped"]},{"attachments":{},"cell_type":"markdown","id":"741a2b61","metadata":{},"source":["Recall that transformations are not actually performed until we trigger an \"action,\" such as one of the following:"]},{"cell_type":"code","execution_count":14,"id":"70dbb666","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["450"]}],"source":["mapped.reduce(lambda a, b: a + b)"]},{"cell_type":"code","execution_count":29,"id":"b90b5a22","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["10"]}],"source":["mapped.count() # also built-in reductions like count and sum"]},{"cell_type":"code","execution_count":13,"id":"9b2b79f2","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[0, 10, 20, 30, 40, 50, 60, 70, 80, 90]"]}],"source":["mapped.collect() # action to bring all data to primary node as list"]},{"cell_type":"code","execution_count":11,"id":"3f9532a9","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[0, 10, 20]"]}],"source":["mapped.take(3) # action to bring select number of data points to primary node as list"]},{"attachments":{},"cell_type":"markdown","id":"1e606a97","metadata":{},"source":["## A Simple Application\n","\n","Reduction by key after mapping/filtering is particularly common workflow. Let's look at a simple application that performs such a transformation. \n","\n","Assume that we have students and grades for two assignments and we want to return the average grade for each student who earned at least a 90 on all assignments, where student grades were stored in a list of tuples:"]},{"cell_type":"code","execution_count":67,"id":"483d4951","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[93.5]"]}],"source":["student_grades = [('John', 90), ('Sue', 95), ('John', 89), ('Sue', 92)]\n","\n","sg_p = sc.parallelize(student_grades) \\\n","         .filter(lambda x: x[1] > 90) \\\n","         .reduceByKey(lambda a, b: (a + b)) \\\n","         .map(lambda x: x[1] / 2) \\\n","         .collect()\n","sg_p"]},{"attachments":{},"cell_type":"markdown","id":"88b81d73","metadata":{},"source":["## Activity 1:\n","\n","Suppose you wanted to parallelize a portion of the Monte Carlo Pi estimation approach we covered earlier in the courseLinks to an external site. using PySpark RDDs and had the following code written thus far:"]},{"cell_type":"code","execution_count":null,"id":"7d058b5f","metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":["import numpy as np\n","\n","n_sims = 100000\n","rand = np.random.uniform(low=-1, high=1, size=(n_sims, 2))"]},{"attachments":{},"cell_type":"markdown","id":"a56338e5","metadata":{},"source":["How would you parallelize the NumPy array rand as an RDD? How would you compute pi using PySpark RDD methods?"]},{"cell_type":"code","execution_count":null,"id":"36e6f29f","metadata":{"vscode":{"languageId":"plaintext"}},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","id":"8be93fc8","metadata":{},"source":["## Activity 2\n","\n","1. Describe in words what is happening in each line of the PySpark code below. At which line will the code actually be executed? Why? You're encouraged to run the code in an EMR cluster and test it out!\n","\n","2. Consider the scalability of this code; if your number of tokens increases (i.e. into the millions) would you want to continue using the `collect` method? Why? What problems might you run into on your EMR cluster?"]},{"cell_type":"code","execution_count":null,"id":"df6f1e21","metadata":{"trusted":true},"outputs":[],"source":["import re\n","tokens = ['cat', 'dog', 'cat', 'rat', 'bat', 'frog', 'cat', 'dog']\n","\n","tokens_p = sc.parallelize(tokens).persist() \\\n","             .map(lambda t: (t, 1)) \\\n","             .filter(lambda t: re.search(r'[at]+', t[0])) \\\n","             .reduceByKey(lambda a, b: a + b) \\\n","             .collect()"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pysparkkernel"},"language_info":{"codemirror_mode":{"name":"python","version":3},"file_extension":".py","mimetype":"text/x-python","name":"pyspark","pygments_lexer":"python3"}},"nbformat":4,"nbformat_minor":5}
